"""ë°ì´í„°ë² ì´ìŠ¤ ë° ì„¤ì • íŒŒì¼ ë°±ì—… ì‹œìŠ¤í…œ."""

import os
import shutil
import gzip
import logging
from pathlib import Path
from datetime import datetime, timedelta
from utils.database import get_connection

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ë°±ì—… ë””ë ‰í† ë¦¬
BACKUP_DIR = Path(__file__).parent.parent.parent / "data" / "backups"
BACKUP_DIR.mkdir(parents=True, exist_ok=True)

# ë°±ì—… ë³´ê´€ ê¸°ê°„ (ì¼)
BACKUP_RETENTION_DAYS = int(os.getenv("BACKUP_RETENTION_DAYS", "30"))


def backup_database():
    """SQLite ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—….
    
    Returns:
        bool: ë°±ì—… ì„±ê³µ ì—¬ë¶€
    """
    try:
        from config import Config
        from utils.database import DB_PATH
        
        if not DB_PATH.exists():
            logger.warning("ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # ë°±ì—… íŒŒì¼ëª…: monitoring_2025-11-22_14-30-45.db.gz
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        backup_file = BACKUP_DIR / f"monitoring_{timestamp}.db.gz"
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (gzip ì••ì¶•)
        with open(DB_PATH, 'rb') as f_in:
            with gzip.open(backup_file, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        file_size = backup_file.stat().st_size / (1024 * 1024)  # MB
        logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: {backup_file.name} ({file_size:.2f}MB)")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤íŒ¨: {str(e)}")
        return False


def backup_config():
    """ì„¤ì • íŒŒì¼ ë°±ì—….
    
    Returns:
        bool: ë°±ì—… ì„±ê³µ ì—¬ë¶€
    """
    try:
        from config import USERS_JSON
        
        if not USERS_JSON.exists():
            logger.warning("ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # ë°±ì—… íŒŒì¼ëª…: users_2025-11-22_14-30-45.json.gz
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        backup_file = BACKUP_DIR / f"users_{timestamp}.json.gz"
        
        # ì„¤ì • íŒŒì¼ ë°±ì—… (gzip ì••ì¶•)
        with open(USERS_JSON, 'rb') as f_in:
            with gzip.open(backup_file, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        file_size = backup_file.stat().st_size / 1024  # KB
        logger.info(f"âœ… ì„¤ì • íŒŒì¼ ë°±ì—… ì™„ë£Œ: {backup_file.name} ({file_size:.2f}KB)")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì„¤ì • íŒŒì¼ ë°±ì—… ì‹¤íŒ¨: {str(e)}")
        return False


def cleanup_old_backups():
    """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì‚­ì œ.
    
    Returns:
        int: ì‚­ì œëœ íŒŒì¼ ìˆ˜
    """
    try:
        deleted_count = 0
        cutoff_date = datetime.now() - timedelta(days=BACKUP_RETENTION_DAYS)
        
        for backup_file in BACKUP_DIR.glob("*.gz"):
            # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
            file_mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
            
            if file_mtime < cutoff_date:
                backup_file.unlink()
                deleted_count += 1
                logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {backup_file.name}")
        
        if deleted_count > 0:
            logger.info(f"âœ… {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ ì™„ë£Œ")
        
        return deleted_count
        
    except Exception as e:
        logger.error(f"âŒ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return 0


def restore_database(backup_file_path):
    """ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬.
    
    Args:
        backup_file_path: ë°±ì—… íŒŒì¼ ê²½ë¡œ (str ë˜ëŠ” Path)
        
    Returns:
        bool: ë³µêµ¬ ì„±ê³µ ì—¬ë¶€
    """
    try:
        from utils.database import DB_PATH
        
        backup_file = Path(backup_file_path)
        
        if not backup_file.exists():
            logger.error(f"ë°±ì—… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {backup_file}")
            return False
        
        # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (ë³µêµ¬ ì‹¤íŒ¨ ì‹œ ë³µì›ìš©)
        if DB_PATH.exists():
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            current_backup = BACKUP_DIR / f"monitoring_current_{timestamp}.db.gz"
            with open(DB_PATH, 'rb') as f_in:
                with gzip.open(current_backup, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
        
        # ë°±ì—… íŒŒì¼ ë³µêµ¬
        with gzip.open(backup_file, 'rb') as f_in:
            with open(DB_PATH, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ì™„ë£Œ: {backup_file.name}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ì‹¤íŒ¨: {str(e)}")
        return False


def get_backup_list():
    """ë°±ì—… íŒŒì¼ ëª©ë¡ ì¡°íšŒ.
    
    Returns:
        list: ë°±ì—… íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    try:
        backups = []
        
        for backup_file in sorted(BACKUP_DIR.glob("*.gz"), reverse=True):
            file_size = backup_file.stat().st_size / (1024 * 1024)  # MB
            file_mtime = datetime.fromtimestamp(backup_file.stat().st_mtime)
            
            backups.append({
                "name": backup_file.name,
                "path": str(backup_file),
                "size_mb": round(file_size, 2),
                "created": file_mtime.isoformat(),
                "type": "database" if "monitoring" in backup_file.name else "config"
            })
        
        return backups
        
    except Exception as e:
        logger.error(f"âŒ ë°±ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return []


def perform_full_backup():
    """ì „ì²´ ë°±ì—… ìˆ˜í–‰.
    
    Returns:
        dict: ë°±ì—… ê²°ê³¼
    """
    logger.info("=" * 70)
    logger.info("ğŸ”„ ì „ì²´ ë°±ì—… ì‹œì‘")
    logger.info("=" * 70)
    
    result = {
        "database": backup_database(),
        "config": backup_config(),
        "cleanup": cleanup_old_backups() >= 0
    }
    
    logger.info("=" * 70)
    if all(result.values()):
        logger.info("âœ… ì „ì²´ ë°±ì—… ì™„ë£Œ")
    else:
        logger.warning("âš ï¸ ì¼ë¶€ ë°±ì—… ì‹¤íŒ¨")
    logger.info("=" * 70)
    
    return result


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš©
    perform_full_backup()
